{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix_V3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNI24GXOEnkU0EEnyFOxAWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haoliu0027/Pix2Pix/blob/master/pix2pix_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlsljA45dI-5",
        "colab_type": "text"
      },
      "source": [
        "# **Linked to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLQVEljDaveo",
        "colab_type": "code",
        "outputId": "dd6760a5-d543-4233-fa86-ad615f3ea391",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.19-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.19-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.19-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj3WT-xobXxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQB4KXlAbgZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/Colab Notebooks/\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MUQn70Vu-Hy",
        "colab_type": "code",
        "outputId": "0ead7077-e895-4dd6-8f4f-fd5b6b944481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " autograd.ipynb\t\t\t\t      pix2pix_group56.ipynb\n",
            " autograd_tutorial.ipynb\t\t      “pix2pix”-Hao\n",
            " checkpoint\t\t\t\t      PIX2PIX_HAO.ipynb\n",
            " CIFAR10_CNN_Ex5_weight_decay_dropout.ipynb   pix2pix_try2.ipynb\n",
            " classification.ipynb\t\t\t      pix2pix_try3.ipynb\n",
            "'CNN_assignment (338f5ae5).ipynb'\t      pytorch_backprop.ipynb\n",
            " CNN_assignment.ipynb\t\t\t      pytorch.nn.ipynb\n",
            " CNN_Mofan.ipynb\t\t\t      Regression.ipynb\n",
            " datasets\t\t\t\t      result\n",
            " datasets1\t\t\t\t      tensor_tutorial.ipynb\n",
            " DL\t\t\t\t\t      “tensor_tutorial.ipynb”的副本\n",
            " “em_assignment.ipynb”的副本\t\t      Untitled\n",
            " Ex4.2_initialization.ipynb\t\t     'Untitled (0cb74ecc)'\n",
            " FeedForwardNetwork.ipynb\t\t      Untitled0.ipynb\n",
            " foundamental.ipynb\t\t\t     'Untitled (9ab13f92)'\n",
            " MNIST_learning_curve.ipynb\t\t      WeightDecay.ipynb\n",
            " MNIST_learning_curve_Tensorboard.ipynb       加速神经网络.ipynb\n",
            " numpy_backprop.ipynb\t\t\t      快速搭建.ipynb\n",
            "'numpy_backprop_(optional).py'\t\t      批训练.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSckpSNdbj6C",
        "colab_type": "code",
        "outputId": "16ca3656-7102-4263-adfb-716846dbe007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " autograd.ipynb\t\t\t\t      pix2pix_group56.ipynb\n",
            " autograd_tutorial.ipynb\t\t      “pix2pix”-Hao\n",
            " checkpoint\t\t\t\t      PIX2PIX_HAO.ipynb\n",
            " CIFAR10_CNN_Ex5_weight_decay_dropout.ipynb   pix2pix_try2.ipynb\n",
            " classification.ipynb\t\t\t      pix2pix_try3.ipynb\n",
            "'CNN_assignment (338f5ae5).ipynb'\t      pytorch_backprop.ipynb\n",
            " CNN_assignment.ipynb\t\t\t      pytorch.nn.ipynb\n",
            " CNN_Mofan.ipynb\t\t\t      Regression.ipynb\n",
            " datasets\t\t\t\t      result\n",
            " datasets1\t\t\t\t      tensor_tutorial.ipynb\n",
            " DL\t\t\t\t\t      “tensor_tutorial.ipynb”的副本\n",
            " “em_assignment.ipynb”的副本\t\t      Untitled\n",
            " Ex4.2_initialization.ipynb\t\t     'Untitled (0cb74ecc)'\n",
            " FeedForwardNetwork.ipynb\t\t      Untitled0.ipynb\n",
            " foundamental.ipynb\t\t\t     'Untitled (9ab13f92)'\n",
            " MNIST_learning_curve.ipynb\t\t      WeightDecay.ipynb\n",
            " MNIST_learning_curve_Tensorboard.ipynb       加速神经网络.ipynb\n",
            " numpy_backprop.ipynb\t\t\t      快速搭建.ipynb\n",
            "'numpy_backprop_(optional).py'\t\t      批训练.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOAN3N3Usj45",
        "colab_type": "text"
      },
      "source": [
        "# **Github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN3XZzcAsnvB",
        "colab_type": "code",
        "outputId": "12f36e26-1e88-4230-d40c-70075c92ca0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " autograd.ipynb\t\t\t\t      pix2pix_group56.ipynb\n",
            " autograd_tutorial.ipynb\t\t      “pix2pix”-Hao\n",
            " checkpoint\t\t\t\t      PIX2PIX_HAO.ipynb\n",
            " CIFAR10_CNN_Ex5_weight_decay_dropout.ipynb   pix2pix_try2.ipynb\n",
            " classification.ipynb\t\t\t      pix2pix_try3.ipynb\n",
            "'CNN_assignment (338f5ae5).ipynb'\t      pytorch_backprop.ipynb\n",
            " CNN_assignment.ipynb\t\t\t      pytorch.nn.ipynb\n",
            " CNN_Mofan.ipynb\t\t\t      Regression.ipynb\n",
            " datasets\t\t\t\t      result\n",
            " datasets1\t\t\t\t      tensor_tutorial.ipynb\n",
            " DL\t\t\t\t\t      “tensor_tutorial.ipynb”的副本\n",
            " “em_assignment.ipynb”的副本\t\t      Untitled\n",
            " Ex4.2_initialization.ipynb\t\t     'Untitled (0cb74ecc)'\n",
            " FeedForwardNetwork.ipynb\t\t      Untitled0.ipynb\n",
            " foundamental.ipynb\t\t\t     'Untitled (9ab13f92)'\n",
            " MNIST_learning_curve.ipynb\t\t      WeightDecay.ipynb\n",
            " MNIST_learning_curve_Tensorboard.ipynb       加速神经网络.ipynb\n",
            " numpy_backprop.ipynb\t\t\t      快速搭建.ipynb\n",
            "'numpy_backprop_(optional).py'\t\t      批训练.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ly0qcYYtsck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SssLpKS5cZ5e",
        "colab_type": "text"
      },
      "source": [
        "# **Library used**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-7PhZpPch8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from math import log10\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "import functools\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ysjxEPzcnVl",
        "colab_type": "text"
      },
      "source": [
        "# **setting parameters**\n",
        "As for dataset, we think the dataformat in the paper is troublesome.So we change the input format. The format is :\n",
        "\n",
        "facades - test - a - b - train - a - b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anbMwSweckZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the dataset in the current directory\n",
        "dataset = 'facades'\n",
        "\n",
        "batch_size = 1\n",
        "test_batch_size=1\n",
        "# direction of the dataset\n",
        "direction='b2a'\n",
        "# number of channels\n",
        "input_nc=3\n",
        "output_nc=3\n",
        "# the number of filters in the first conv layer\n",
        "ngf=64\n",
        "ndf=64\n",
        "\n",
        "epoch_count=1\n",
        "niter=100\n",
        "niter_decay=100\n",
        "\n",
        "lr=0.0002\n",
        "lr_policy='lambda'\n",
        "lr_decay_iters=30\n",
        "beta1=0.5\n",
        "\n",
        "threads=0\n",
        "seed=123\n",
        "lamb=10\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "# this is the cpu version\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "# if using the gpu, open the following code\n",
        "torch.cuda.manual_seed(seed)\n",
        "device = torch.device(\"cuda: 0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt_2JQtbdeii",
        "colab_type": "text"
      },
      "source": [
        "# **Funtions for loading dataset and preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HSvdjffdaVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n",
        "\n",
        "\n",
        "# Inherit the data.Dataset and create a new dataset for getting each item easily\n",
        "class DatasetFromFolder(data.Dataset):\n",
        "    \n",
        "    def __init__(self, image_dir, direction):\n",
        "        super(DatasetFromFolder, self).__init__()\n",
        "        self.direction = direction\n",
        "        self.a_path = join(image_dir, \"a\")\n",
        "        self.b_path = join(image_dir, \"b\")\n",
        "        self.image_filenames = [x for x in listdir(self.a_path) if is_image_file(x)]\n",
        "\n",
        "        transform_list = [transforms.ToTensor(),\n",
        "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "        self.transform = transforms.Compose(transform_list)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # covert into RGB picture, do they cover the original pictures?\n",
        "        # Answer: this is just converting the original pictures \n",
        "        # join : combine each paths in the list \n",
        "        # Preprocessing of the pictures\n",
        "        a = Image.open(join(self.a_path, self.image_filenames[index])).convert('RGB')\n",
        "        b = Image.open(join(self.b_path, self.image_filenames[index])).convert('RGB')\n",
        "        # Resize\n",
        "        a = a.resize((286, 286), Image.BICUBIC)\n",
        "        b = b.resize((286, 286), Image.BICUBIC)\n",
        "        # To tensor\n",
        "        a = transforms.ToTensor()(a)\n",
        "        b = transforms.ToTensor()(b)\n",
        "        # add a offset to the picture\n",
        "        w_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
        "        h_offset = random.randint(0, max(0, 286 - 256 - 1))\n",
        "    \n",
        "        a = a[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
        "        b = b[:, h_offset:h_offset + 256, w_offset:w_offset + 256]\n",
        "        # Normalize\n",
        "        a = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(a)\n",
        "        b = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(b)\n",
        "        \n",
        "      \n",
        "        if random.random() < 0.5:\n",
        "            idx = [i for i in range(a.size(2) - 1, -1, -1)]\n",
        "            idx = torch.LongTensor(idx)\n",
        "            a = a.index_select(2, idx)\n",
        "            b = b.index_select(2, idx)\n",
        "\n",
        "        if self.direction == \"a2b\":\n",
        "            return a, b\n",
        "        else:\n",
        "            return b, a\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9_H6Ypednp3",
        "colab_type": "text"
      },
      "source": [
        "# **Design Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5KZZkF1dmpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_nc, ngf, norm_layer, use_bias\n",
        "# reflectionPad2d : 使用的填充宽度是3，填充结果和原来的数组相比宽、高都要加4，这一点是没问题的，\n",
        "# 只不过并不是使用0来填充。如果仔细观察的话，可以发现使用的是镜像填充的方法，\n",
        "# 以边界的一行或者一列为对称轴，就可以发现两规律了\n",
        "class Inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n",
        "        super(Inconv, self).__init__()\n",
        "        self.inconv = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0,\n",
        "                      bias=use_bias),\n",
        "            norm_layer(out_ch),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.inconv(x)\n",
        "        return x\n",
        "# ngf, ngf * 2, norm_layer, use_bias\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n",
        "        super(Down, self).__init__()\n",
        "        self.down = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3,\n",
        "                      stride=2, padding=1, bias=use_bias),\n",
        "            norm_layer(out_ch),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, norm_layer, use_bias):\n",
        "        super(Up, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            # nn.Conv2d(in_ch, out_ch,\n",
        "            #           kernel_size=3, stride=1,\n",
        "            #           padding=1, bias=use_bias),\n",
        "            nn.ConvTranspose2d(in_ch, out_ch,\n",
        "                               kernel_size=3, stride=2,\n",
        "                               padding=1, output_padding=1,\n",
        "                               bias=use_bias),\n",
        "            norm_layer(out_ch),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Outconv, self).__init__()\n",
        "        self.outconv = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=7, padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.outconv(x)\n",
        "        return x\n",
        "\n",
        "def init_net(net, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        net.to(gpu_id)\n",
        "        init_weights(net, init_type, gain=init_gain)\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sc8NqF2dxUL",
        "colab_type": "text"
      },
      "source": [
        "# **5. gernerator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lza8y54bdv-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    \"\"\"Defines the Unet submodule with skip connection.\n",
        "        X -------------------identity----------------------\n",
        "        |-- downsampling -- |submodule| -- upsampling --|\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        \"\"\"Construct a Unet submodule with skip connections.\n",
        "\n",
        "        Parameters:\n",
        "            outer_nc (int) -- the number of filters in the outer conv layer\n",
        "            inner_nc (int) -- the number of filters in the inner conv layer\n",
        "            input_nc (int) -- the number of channels in input images/features\n",
        "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
        "            outermost (bool)    -- if this module is the outermost module\n",
        "            innermost (bool)    -- if this module is the innermost module\n",
        "            norm_layer          -- normalization layer\n",
        "            use_dropout (bool)  -- if use dropout layers.\n",
        "        \"\"\"\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=use_bias)\n",
        "#对输入的每一个元素运用$f(x) = max(0, x) + {negative_slope} * min(0, x)$\n",
        "#参数：\n",
        "    # negative_slope：控制负斜率的角度，默认等于0.01\n",
        "    # inplace-选择是否进行覆盖运算\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:   # add skip connections\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "    \n",
        "\n",
        "    \n",
        "class UnetGenerator(nn.Module):\n",
        "    \"\"\"Create a Unet-based generator\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        \"\"\"Construct a Unet generator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            output_nc (int) -- the number of channels in output images\n",
        "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
        "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
        "            ngf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "\n",
        "        We construct the U-Net from the innermost layer to the outermost layer.\n",
        "        It is a recursive process.\n",
        "        \"\"\"\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, \n",
        "                                             norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
        "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, \n",
        "                                                 norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, \n",
        "                                             norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, \n",
        "                                             norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, \n",
        "                                             norm_layer=norm_layer)\n",
        "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, \n",
        "                                             outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward\"\"\"\n",
        "        return self.model(input)\n",
        "\n",
        "    \n",
        "def define_G(input_nc, output_nc, ngf, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n",
        "    net = None\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\n",
        "\n",
        "#     net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
        "    net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "    return init_net(net, init_type, init_gain, gpu_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHtM-TFQd7Pn",
        "colab_type": "text"
      },
      "source": [
        "# **6. Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnOAm2fPd6hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        kw = 4\n",
        "        padw = 1\n",
        "        sequence = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n",
        "\n",
        "        if use_sigmoid:\n",
        "            sequence += [nn.Sigmoid()]\n",
        "\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class PixelDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "\n",
        "        if use_sigmoid:\n",
        "            self.net.append(nn.Sigmoid())\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.net(input)\n",
        "    \n",
        "def define_D(input_nc, ndf, netD,\n",
        "             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='normal', init_gain=0.02, gpu_id='cuda:0'):\n",
        "    net = None\n",
        "    norm_layer = get_norm_layer(norm_type=norm)\n",
        "\n",
        "    if netD == 'basic':\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n",
        "    elif netD == 'n_layers':\n",
        "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n",
        "    elif netD == 'pixel':\n",
        "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n",
        "    else:\n",
        "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % net)\n",
        "\n",
        "    return init_net(net, init_type, init_gain, gpu_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHP48HwjeE1y",
        "colab_type": "text"
      },
      "source": [
        "# **7. init_param, loss fn, optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx64qt1peDx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)\n",
        "\n",
        "\n",
        "\n",
        "def get_norm_layer(norm_type='instance'):\n",
        "    if norm_type == 'batch':\n",
        "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
        "    elif norm_type == 'switchable':\n",
        "        norm_layer = SwitchNorm2d\n",
        "    elif norm_type == 'none':\n",
        "        norm_layer = None\n",
        "    else:\n",
        "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
        "    return norm_layer\n",
        "\n",
        "\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        if use_lsgan:\n",
        "            self.loss = nn.MSELoss()\n",
        "        else:\n",
        "            # binary cross entropy\n",
        "            self.loss = nn.BCELoss()\n",
        "\n",
        "    def get_target_tensor(self, input, target_is_real):\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(input)\n",
        "\n",
        "    def __call__(self, input, target_is_real):\n",
        "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
        "        return self.loss(input, target_tensor)\n",
        "    \n",
        "def update_learning_rate(scheduler, optimizer):\n",
        "    scheduler.step()\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print('learning rate = %.7f' % lr)  \n",
        "\n",
        "# learning rate decay\n",
        "def get_scheduler(optimizer):\n",
        "    if lr_policy == 'lambda':\n",
        "        def lambda_rule(epoch):\n",
        "            lr_l = 1.0 - max(0, epoch + epoch_count - niter) / float(niter_decay + 1)\n",
        "            return lr_l\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    elif lr_policy == 'step':\n",
        "        scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_decay_iters, gamma=0.1)\n",
        "    elif lr_policy == 'plateau':\n",
        "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
        "    elif lr_policy == 'cosine':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n",
        "    else:\n",
        "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
        "    return scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGzvrtKdeSKy",
        "colab_type": "text"
      },
      "source": [
        "# **8. train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFqAgkeDeRdo",
        "colab_type": "code",
        "outputId": "0b3fe76e-bd43-42c6-88fe-d480c8623a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dataroot1 = \"datasets/facades/train\"\n",
        "dataroot2 = \"datasets/facades/test\"\n",
        "# As for windows, delete num_works parameter\n",
        "training_data_loader = DataLoader(dataset=DatasetFromFolder(dataroot1, direction), num_workers=threads, batch_size=batch_size, shuffle=True)\n",
        "testing_data_loader = DataLoader(dataset=DatasetFromFolder(dataroot2, direction), num_workers=threads, batch_size=test_batch_size)\n",
        "\n",
        "\n",
        "print('===> Building models')\n",
        "\n",
        "'''loading the generator and discriminator'''\n",
        "net_g = define_G(input_nc, output_nc, ngf, 'batch', False, 'normal', 0.02, gpu_id=device)\n",
        "net_d = define_D(input_nc + output_nc, ndf, 'basic', gpu_id=device)\n",
        "\n",
        "'''set loss fn'''\n",
        "criterionGAN = GANLoss().to(device)\n",
        "criterionL1 = nn.L1Loss().to(device)\n",
        "criterionMSE = nn.MSELoss().to(device)\n",
        "\n",
        "'''setup optimizer''' \n",
        "optimizer_g = optim.Adam(net_g.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_d = optim.Adam(net_d.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "'''set the learning rate adjust policy'''\n",
        "net_g_scheduler = get_scheduler(optimizer_g)\n",
        "net_d_scheduler = get_scheduler(optimizer_d)\n",
        "\n",
        "'''training process'''\n",
        "for epoch in range(epoch_count, niter + niter_decay + 1):\n",
        "    \n",
        "    for iteration, batch in enumerate(training_data_loader, 1):\n",
        "        # forward\n",
        "        real_a, real_b = batch[0].to(device), batch[1].to(device)\n",
        "        fake_b = net_g(real_a)\n",
        "\n",
        "        ######################\n",
        "        # (1) Update D network\n",
        "        ######################\n",
        "\n",
        "        optimizer_d.zero_grad()\n",
        "        \n",
        "        # D train with fake\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = net_d.forward(fake_ab.detach())\n",
        "        loss_d_fake = criterionGAN(pred_fake, False)\n",
        "\n",
        "        # D train with real\n",
        "        real_ab = torch.cat((real_a, real_b), 1)\n",
        "        pred_real = net_d.forward(real_ab)\n",
        "        loss_d_real = criterionGAN(pred_real, True)\n",
        "        \n",
        "        # Combined D loss\n",
        "        loss_d = (loss_d_fake + loss_d_real) * 0.5\n",
        "\n",
        "        loss_d.backward()\n",
        "       \n",
        "        optimizer_d.step()\n",
        "\n",
        "        ######################\n",
        "        # (2) Update G network\n",
        "        ######################\n",
        "\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_ab = torch.cat((real_a, fake_b), 1)\n",
        "        pred_fake = net_d.forward(fake_ab)\n",
        "        loss_g_gan = criterionGAN(pred_fake, True)\n",
        "\n",
        "        # Second, G(A) = B\n",
        "        loss_g_l1 = criterionL1(fake_b, real_b) * lamb\n",
        "        \n",
        "        loss_g = loss_g_gan + loss_g_l1\n",
        "        \n",
        "        loss_g.backward()\n",
        "\n",
        "        optimizer_g.step()\n",
        "\n",
        "        print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f}\".format(\n",
        "            epoch, iteration, len(training_data_loader), loss_d.item(), loss_g.item()))\n",
        "\n",
        "    update_learning_rate(net_g_scheduler, optimizer_g)\n",
        "    update_learning_rate(net_d_scheduler, optimizer_d)\n",
        "\n",
        "    # test\n",
        "    avg_psnr = 0\n",
        "    for batch in testing_data_loader:\n",
        "        input, target = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        prediction = net_g(input)\n",
        "        mse = criterionMSE(prediction, target)\n",
        "        psnr = 10 * log10(1 / mse.item())\n",
        "        avg_psnr += psnr\n",
        "    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n",
        "\n",
        "    #checkpoint\n",
        "    if epoch % 50 == 0:\n",
        "        if not os.path.exists(\"checkpoint\"):\n",
        "            os.mkdir(\"checkpoint\")\n",
        "        if not os.path.exists(os.path.join(\"checkpoint\", dataset)):\n",
        "            os.mkdir(os.path.join(\"checkpoint\", dataset))\n",
        "        net_g_model_out_path = \"checkpoint/{}/netG_model_epoch_{}.pth\".format(dataset, epoch)\n",
        "        net_d_model_out_path = \"checkpoint/{}/netD_model_epoch_{}.pth\".format(dataset, epoch)\n",
        "        torch.save(net_g, net_g_model_out_path)\n",
        "        torch.save(net_d, net_d_model_out_path)\n",
        "        print(\"Checkpoint saved to {}\".format(\"checkpoint\" + dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===> Building models\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "===> Epoch[1](1/394): Loss_D: 2.2437 Loss_G: 10.6741\n",
            "===> Epoch[1](2/394): Loss_D: 3.8317 Loss_G: 11.9552\n",
            "===> Epoch[1](3/394): Loss_D: 4.1220 Loss_G: 11.5479\n",
            "===> Epoch[1](4/394): Loss_D: 3.7632 Loss_G: 7.9851\n",
            "===> Epoch[1](5/394): Loss_D: 2.9021 Loss_G: 7.6858\n",
            "===> Epoch[1](6/394): Loss_D: 2.0041 Loss_G: 5.7446\n",
            "===> Epoch[1](7/394): Loss_D: 1.5035 Loss_G: 5.7457\n",
            "===> Epoch[1](8/394): Loss_D: 1.6186 Loss_G: 4.5810\n",
            "===> Epoch[1](9/394): Loss_D: 1.2183 Loss_G: 4.1918\n",
            "===> Epoch[1](10/394): Loss_D: 1.0031 Loss_G: 5.4067\n",
            "===> Epoch[1](11/394): Loss_D: 1.0752 Loss_G: 4.7914\n",
            "===> Epoch[1](12/394): Loss_D: 1.1385 Loss_G: 4.6618\n",
            "===> Epoch[1](13/394): Loss_D: 1.0034 Loss_G: 4.7821\n",
            "===> Epoch[1](14/394): Loss_D: 0.9042 Loss_G: 4.4006\n",
            "===> Epoch[1](15/394): Loss_D: 0.8716 Loss_G: 4.7298\n",
            "===> Epoch[1](16/394): Loss_D: 0.8563 Loss_G: 4.9327\n",
            "===> Epoch[1](17/394): Loss_D: 0.9237 Loss_G: 4.3298\n",
            "===> Epoch[1](18/394): Loss_D: 1.4399 Loss_G: 7.5458\n",
            "===> Epoch[1](19/394): Loss_D: 1.2498 Loss_G: 6.1580\n",
            "===> Epoch[1](20/394): Loss_D: 2.6854 Loss_G: 11.8093\n",
            "===> Epoch[1](21/394): Loss_D: 2.3927 Loss_G: 5.8759\n",
            "===> Epoch[1](22/394): Loss_D: 2.2308 Loss_G: 5.4692\n",
            "===> Epoch[1](23/394): Loss_D: 1.2390 Loss_G: 4.4720\n",
            "===> Epoch[1](24/394): Loss_D: 1.6910 Loss_G: 7.9058\n",
            "===> Epoch[1](25/394): Loss_D: 3.1011 Loss_G: 10.4435\n",
            "===> Epoch[1](26/394): Loss_D: 1.1802 Loss_G: 6.5682\n",
            "===> Epoch[1](27/394): Loss_D: 1.6294 Loss_G: 4.8414\n",
            "===> Epoch[1](28/394): Loss_D: 1.1580 Loss_G: 4.0866\n",
            "===> Epoch[1](29/394): Loss_D: 1.0037 Loss_G: 3.7409\n",
            "===> Epoch[1](30/394): Loss_D: 1.0298 Loss_G: 4.1582\n",
            "===> Epoch[1](31/394): Loss_D: 0.8929 Loss_G: 5.0516\n",
            "===> Epoch[1](32/394): Loss_D: 0.8737 Loss_G: 4.5834\n",
            "===> Epoch[1](33/394): Loss_D: 0.8676 Loss_G: 4.1954\n",
            "===> Epoch[1](34/394): Loss_D: 0.6383 Loss_G: 5.1402\n",
            "===> Epoch[1](35/394): Loss_D: 0.8420 Loss_G: 6.8983\n",
            "===> Epoch[1](36/394): Loss_D: 0.9035 Loss_G: 3.2405\n",
            "===> Epoch[1](37/394): Loss_D: 0.6603 Loss_G: 3.5884\n",
            "===> Epoch[1](38/394): Loss_D: 0.7488 Loss_G: 4.0243\n",
            "===> Epoch[1](39/394): Loss_D: 0.7935 Loss_G: 3.9685\n",
            "===> Epoch[1](40/394): Loss_D: 0.6816 Loss_G: 4.9644\n",
            "===> Epoch[1](41/394): Loss_D: 0.7058 Loss_G: 3.6298\n",
            "===> Epoch[1](42/394): Loss_D: 0.5545 Loss_G: 4.0082\n",
            "===> Epoch[1](43/394): Loss_D: 0.5177 Loss_G: 3.7115\n",
            "===> Epoch[1](44/394): Loss_D: 0.5714 Loss_G: 3.9741\n",
            "===> Epoch[1](45/394): Loss_D: 0.5121 Loss_G: 5.4157\n",
            "===> Epoch[1](46/394): Loss_D: 0.6540 Loss_G: 4.5128\n",
            "===> Epoch[1](47/394): Loss_D: 0.5579 Loss_G: 4.3875\n",
            "===> Epoch[1](48/394): Loss_D: 0.6148 Loss_G: 3.4981\n",
            "===> Epoch[1](49/394): Loss_D: 0.5570 Loss_G: 3.7282\n",
            "===> Epoch[1](50/394): Loss_D: 0.5982 Loss_G: 4.3082\n",
            "===> Epoch[1](51/394): Loss_D: 0.4702 Loss_G: 5.0069\n",
            "===> Epoch[1](52/394): Loss_D: 0.5184 Loss_G: 4.7390\n",
            "===> Epoch[1](53/394): Loss_D: 0.5615 Loss_G: 3.4014\n",
            "===> Epoch[1](54/394): Loss_D: 0.6261 Loss_G: 3.7304\n",
            "===> Epoch[1](55/394): Loss_D: 0.7533 Loss_G: 4.1602\n",
            "===> Epoch[1](56/394): Loss_D: 0.5855 Loss_G: 3.3462\n",
            "===> Epoch[1](57/394): Loss_D: 0.6029 Loss_G: 4.8625\n",
            "===> Epoch[1](58/394): Loss_D: 0.6035 Loss_G: 4.3798\n",
            "===> Epoch[1](59/394): Loss_D: 0.6488 Loss_G: 6.9550\n",
            "===> Epoch[1](60/394): Loss_D: 1.0029 Loss_G: 4.4975\n",
            "===> Epoch[1](61/394): Loss_D: 0.5407 Loss_G: 4.5110\n",
            "===> Epoch[1](62/394): Loss_D: 0.4830 Loss_G: 4.6190\n",
            "===> Epoch[1](63/394): Loss_D: 0.7466 Loss_G: 7.4960\n",
            "===> Epoch[1](64/394): Loss_D: 0.8401 Loss_G: 6.3338\n",
            "===> Epoch[1](65/394): Loss_D: 0.6672 Loss_G: 4.7514\n",
            "===> Epoch[1](66/394): Loss_D: 0.7397 Loss_G: 4.2221\n",
            "===> Epoch[1](67/394): Loss_D: 0.5867 Loss_G: 5.4404\n",
            "===> Epoch[1](68/394): Loss_D: 0.6428 Loss_G: 3.7904\n",
            "===> Epoch[1](69/394): Loss_D: 0.6971 Loss_G: 3.8789\n",
            "===> Epoch[1](70/394): Loss_D: 0.4491 Loss_G: 3.8304\n",
            "===> Epoch[1](71/394): Loss_D: 0.6234 Loss_G: 4.2028\n",
            "===> Epoch[1](72/394): Loss_D: 0.6843 Loss_G: 4.0520\n",
            "===> Epoch[1](73/394): Loss_D: 0.6526 Loss_G: 5.2534\n",
            "===> Epoch[1](74/394): Loss_D: 0.6328 Loss_G: 3.4491\n",
            "===> Epoch[1](75/394): Loss_D: 0.7465 Loss_G: 4.2708\n",
            "===> Epoch[1](76/394): Loss_D: 0.7152 Loss_G: 6.1556\n",
            "===> Epoch[1](77/394): Loss_D: 0.7811 Loss_G: 5.1153\n",
            "===> Epoch[1](78/394): Loss_D: 0.6693 Loss_G: 5.2445\n",
            "===> Epoch[1](79/394): Loss_D: 1.1040 Loss_G: 5.1850\n",
            "===> Epoch[1](80/394): Loss_D: 1.0799 Loss_G: 6.6457\n",
            "===> Epoch[1](81/394): Loss_D: 1.0935 Loss_G: 4.8890\n",
            "===> Epoch[1](82/394): Loss_D: 2.5968 Loss_G: 8.8493\n",
            "===> Epoch[1](83/394): Loss_D: 2.3319 Loss_G: 5.8112\n",
            "===> Epoch[1](84/394): Loss_D: 1.9780 Loss_G: 5.4687\n",
            "===> Epoch[1](85/394): Loss_D: 1.0269 Loss_G: 5.1361\n",
            "===> Epoch[1](86/394): Loss_D: 0.7760 Loss_G: 4.1214\n",
            "===> Epoch[1](87/394): Loss_D: 1.2095 Loss_G: 6.1191\n",
            "===> Epoch[1](88/394): Loss_D: 0.9854 Loss_G: 5.4811\n",
            "===> Epoch[1](89/394): Loss_D: 1.1730 Loss_G: 6.2390\n",
            "===> Epoch[1](90/394): Loss_D: 1.1068 Loss_G: 3.6096\n",
            "===> Epoch[1](91/394): Loss_D: 1.2954 Loss_G: 4.6681\n",
            "===> Epoch[1](92/394): Loss_D: 1.0408 Loss_G: 3.6502\n",
            "===> Epoch[1](93/394): Loss_D: 0.5363 Loss_G: 2.8077\n",
            "===> Epoch[1](94/394): Loss_D: 0.5958 Loss_G: 3.0358\n",
            "===> Epoch[1](95/394): Loss_D: 0.6695 Loss_G: 6.0304\n",
            "===> Epoch[1](96/394): Loss_D: 0.6155 Loss_G: 2.9778\n",
            "===> Epoch[1](97/394): Loss_D: 0.5770 Loss_G: 4.0508\n",
            "===> Epoch[1](98/394): Loss_D: 0.5161 Loss_G: 4.2005\n",
            "===> Epoch[1](99/394): Loss_D: 0.5621 Loss_G: 5.1234\n",
            "===> Epoch[1](100/394): Loss_D: 0.5180 Loss_G: 5.1996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-46c95a936e4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mniter_decay\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mreal_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3c2bcb9c20b2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Preprocessing of the pictures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m286\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m286\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Yditn6ioH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrXDAAWsecET",
        "colab_type": "text"
      },
      "source": [
        "# **9. Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcgS1fVreZhC",
        "colab_type": "code",
        "outputId": "035f21b4-1252-4bbf-869c-cf6e81313bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def load_img(filepath):\n",
        "    img = Image.open(filepath).convert('RGB')\n",
        "    img = img.resize((256, 256), Image.BICUBIC)\n",
        "    return img\n",
        "\n",
        "def save_img(image_tensor, filename):\n",
        "    image_numpy = image_tensor.float().numpy()\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    image_numpy = image_numpy.clip(0, 255)\n",
        "    image_numpy = image_numpy.astype(np.uint8)\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(filename)\n",
        "    print(\"Image saved as {}\".format(filename))\n",
        "\n",
        "nepochs = 150\n",
        "model_path = \"checkpoint/{}/netG_model_epoch_{}.pth\".format(dataset, nepochs)\n",
        "\n",
        "net_g = torch.load(model_path).to(device)\n",
        "\n",
        "if direction == \"a2b\":\n",
        "    image_dir = \"datasets/{}/test/a/\".format(dataset)\n",
        "else:\n",
        "    image_dir = \"datasets/{}/test/b/\".format(dataset)\n",
        "\n",
        "image_filenames = [x for x in os.listdir(image_dir) if is_image_file(x)]\n",
        "\n",
        "transform_list = [transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "\n",
        "transform = transforms.Compose(transform_list)\n",
        "\n",
        "for image_name in image_filenames:\n",
        "    img = load_img(image_dir + image_name)\n",
        "    img = transform(img)\n",
        "    input = img.unsqueeze(0).to(device)\n",
        "    out = net_g(input)\n",
        "    out_img = out.detach().squeeze(0).cpu()\n",
        "\n",
        "    if not os.path.exists(os.path.join(\"result\", dataset)):\n",
        "        os.makedirs(os.path.join(\"result\", dataset))\n",
        "    save_img(out_img, \"result/{}/{}\".format(dataset, image_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image saved as result/facades/cmp_b0313.png\n",
            "Image saved as result/facades/cmp_b0343.png\n",
            "Image saved as result/facades/cmp_b0334.png\n",
            "Image saved as result/facades/cmp_b0319.png\n",
            "Image saved as result/facades/cmp_b0340.png\n",
            "Image saved as result/facades/cmp_b0341.png\n",
            "Image saved as result/facades/cmp_b0315.png\n",
            "Image saved as result/facades/cmp_b0325.png\n",
            "Image saved as result/facades/cmp_b0345.png\n",
            "Image saved as result/facades/cmp_b0364.png\n",
            "Image saved as result/facades/cmp_b0365.png\n",
            "Image saved as result/facades/cmp_b0352.png\n",
            "Image saved as result/facades/cmp_b0355.png\n",
            "Image saved as result/facades/cmp_b0353.png\n",
            "Image saved as result/facades/cmp_b0368.png\n",
            "Image saved as result/facades/cmp_b0360.png\n",
            "Image saved as result/facades/cmp_b0349.png\n",
            "Image saved as result/facades/cmp_b0363.png\n",
            "Image saved as result/facades/cmp_b0370.png\n",
            "Image saved as result/facades/cmp_x0006.png\n",
            "Image saved as result/facades/cmp_b0377.png\n",
            "Image saved as result/facades/cmp_b0373.png\n",
            "Image saved as result/facades/cmp_b0372.png\n",
            "Image saved as result/facades/cmp_x0003.png\n",
            "Image saved as result/facades/cmp_x0009.png\n",
            "Image saved as result/facades/cmp_x0007.png\n",
            "Image saved as result/facades/cmp_x0011.png\n",
            "Image saved as result/facades/cmp_b0376.png\n",
            "Image saved as result/facades/cmp_x0013.png\n",
            "Image saved as result/facades/cmp_x0039.png\n",
            "Image saved as result/facades/cmp_x0033.png\n",
            "Image saved as result/facades/cmp_x0032.png\n",
            "Image saved as result/facades/cmp_x0041.png\n",
            "Image saved as result/facades/cmp_x0027.png\n",
            "Image saved as result/facades/cmp_x0040.png\n",
            "Image saved as result/facades/cmp_x0024.png\n",
            "Image saved as result/facades/cmp_x0023.png\n",
            "Image saved as result/facades/cmp_x0026.png\n",
            "Image saved as result/facades/cmp_x0046.png\n",
            "Image saved as result/facades/cmp_x0059.png\n",
            "Image saved as result/facades/cmp_x0070.png\n",
            "Image saved as result/facades/cmp_x0073.png\n",
            "Image saved as result/facades/cmp_x0060.png\n",
            "Image saved as result/facades/cmp_x0047.png\n",
            "Image saved as result/facades/cmp_x0077.png\n",
            "Image saved as result/facades/cmp_x0053.png\n",
            "Image saved as result/facades/cmp_x0076.png\n",
            "Image saved as result/facades/cmp_x0057.png\n",
            "Image saved as result/facades/cmp_x0080.png\n",
            "Image saved as result/facades/cmp_x0096.png\n",
            "Image saved as result/facades/cmp_x0101.png\n",
            "Image saved as result/facades/cmp_x0100.png\n",
            "Image saved as result/facades/cmp_x0105.png\n",
            "Image saved as result/facades/cmp_x0084.png\n",
            "Image saved as result/facades/cmp_x0097.png\n",
            "Image saved as result/facades/cmp_x0093.png\n",
            "Image saved as result/facades/cmp_x0083.png\n",
            "Image saved as result/facades/cmp_x0086.png\n",
            "Image saved as result/facades/cmp_x0107.png\n",
            "Image saved as result/facades/cmp_x0116.png\n",
            "Image saved as result/facades/cmp_x0119.png\n",
            "Image saved as result/facades/cmp_x0129.png\n",
            "Image saved as result/facades/cmp_x0117.png\n",
            "Image saved as result/facades/cmp_x0130.png\n",
            "Image saved as result/facades/cmp_x0128.png\n",
            "Image saved as result/facades/cmp_x0127.png\n",
            "Image saved as result/facades/cmp_x0114.png\n",
            "Image saved as result/facades/cmp_x0126.png\n",
            "Image saved as result/facades/cmp_x0133.png\n",
            "Image saved as result/facades/cmp_x0156.png\n",
            "Image saved as result/facades/cmp_x0160.png\n",
            "Image saved as result/facades/cmp_x0143.png\n",
            "Image saved as result/facades/cmp_x0147.png\n",
            "Image saved as result/facades/cmp_x0142.png\n",
            "Image saved as result/facades/cmp_x0157.png\n",
            "Image saved as result/facades/cmp_x0159.png\n",
            "Image saved as result/facades/cmp_x0151.png\n",
            "Image saved as result/facades/cmp_x0161.png\n",
            "Image saved as result/facades/cmp_x0163.png\n",
            "Image saved as result/facades/cmp_x0164.png\n",
            "Image saved as result/facades/cmp_x0198.png\n",
            "Image saved as result/facades/cmp_x0177.png\n",
            "Image saved as result/facades/cmp_x0184.png\n",
            "Image saved as result/facades/cmp_x0183.png\n",
            "Image saved as result/facades/cmp_x0188.png\n",
            "Image saved as result/facades/cmp_x0169.png\n",
            "Image saved as result/facades/cmp_x0186.png\n",
            "Image saved as result/facades/cmp_x0197.png\n",
            "Image saved as result/facades/cmp_x0205.png\n",
            "Image saved as result/facades/cmp_x0223.png\n",
            "Image saved as result/facades/cmp_x0221.png\n",
            "Image saved as result/facades/cmp_x0214.png\n",
            "Image saved as result/facades/cmp_x0220.png\n",
            "Image saved as result/facades/cmp_x0216.png\n",
            "Image saved as result/facades/cmp_x0207.png\n",
            "Image saved as result/facades/cmp_x0213.png\n",
            "Image saved as result/facades/cmp_x0210.png\n",
            "Image saved as result/facades/cmp_x0217.png\n",
            "Image saved as result/facades/cmp_x0224.png\n",
            "Image saved as result/facades/cmp_x0226.png\n",
            "Image saved as result/facades/cmp_b0035.png\n",
            "Image saved as result/facades/cmp_b0034.png\n",
            "Image saved as result/facades/cmp_b0023.png\n",
            "Image saved as result/facades/cmp_b0055.png\n",
            "Image saved as result/facades/cmp_b0054.png\n",
            "Image saved as result/facades/cmp_b0026.png\n",
            "Image saved as result/facades/cmp_b0049.png\n",
            "Image saved as result/facades/cmp_b0047.png\n",
            "Image saved as result/facades/cmp_b0057.png\n",
            "Image saved as result/facades/cmp_b0068.png\n",
            "Image saved as result/facades/cmp_b0075.png\n",
            "Image saved as result/facades/cmp_b0084.png\n",
            "Image saved as result/facades/cmp_b0061.png\n",
            "Image saved as result/facades/cmp_b0081.png\n",
            "Image saved as result/facades/cmp_b0071.png\n",
            "Image saved as result/facades/cmp_b0073.png\n",
            "Image saved as result/facades/cmp_b0078.png\n",
            "Image saved as result/facades/cmp_b0077.png\n",
            "Image saved as result/facades/cmp_b0085.png\n",
            "Image saved as result/facades/cmp_b0090.png\n",
            "Image saved as result/facades/cmp_b0112.png\n",
            "Image saved as result/facades/cmp_b0103.png\n",
            "Image saved as result/facades/cmp_b0095.png\n",
            "Image saved as result/facades/cmp_b0089.png\n",
            "Image saved as result/facades/cmp_b0087.png\n",
            "Image saved as result/facades/cmp_b0093.png\n",
            "Image saved as result/facades/cmp_b0105.png\n",
            "Image saved as result/facades/cmp_b0096.png\n",
            "Image saved as result/facades/cmp_b0113.png\n",
            "Image saved as result/facades/cmp_b0122.png\n",
            "Image saved as result/facades/cmp_b0145.png\n",
            "Image saved as result/facades/cmp_b0125.png\n",
            "Image saved as result/facades/cmp_b0121.png\n",
            "Image saved as result/facades/cmp_b0126.png\n",
            "Image saved as result/facades/cmp_b0141.png\n",
            "Image saved as result/facades/cmp_b0124.png\n",
            "Image saved as result/facades/cmp_b0135.png\n",
            "Image saved as result/facades/cmp_b0127.png\n",
            "Image saved as result/facades/cmp_b0146.png\n",
            "Image saved as result/facades/cmp_b0162.png\n",
            "Image saved as result/facades/cmp_b0149.png\n",
            "Image saved as result/facades/cmp_b0154.png\n",
            "Image saved as result/facades/cmp_b0156.png\n",
            "Image saved as result/facades/cmp_b0164.png\n",
            "Image saved as result/facades/cmp_b0148.png\n",
            "Image saved as result/facades/cmp_b0165.png\n",
            "Image saved as result/facades/cmp_b0157.png\n",
            "Image saved as result/facades/cmp_b0150.png\n",
            "Image saved as result/facades/cmp_b0166.png\n",
            "Image saved as result/facades/cmp_b0187.png\n",
            "Image saved as result/facades/cmp_b0168.png\n",
            "Image saved as result/facades/cmp_b0167.png\n",
            "Image saved as result/facades/cmp_b0172.png\n",
            "Image saved as result/facades/cmp_b0188.png\n",
            "Image saved as result/facades/cmp_b0184.png\n",
            "Image saved as result/facades/cmp_b0174.png\n",
            "Image saved as result/facades/cmp_b0190.png\n",
            "Image saved as result/facades/cmp_b0185.png\n",
            "Image saved as result/facades/cmp_b0192.png\n",
            "Image saved as result/facades/cmp_b0207.png\n",
            "Image saved as result/facades/cmp_b0200.png\n",
            "Image saved as result/facades/cmp_b0214.png\n",
            "Image saved as result/facades/cmp_b0202.png\n",
            "Image saved as result/facades/cmp_b0218.png\n",
            "Image saved as result/facades/cmp_b0204.png\n",
            "Image saved as result/facades/cmp_b0199.png\n",
            "Image saved as result/facades/cmp_b0206.png\n",
            "Image saved as result/facades/cmp_b0219.png\n",
            "Image saved as result/facades/cmp_b0227.png\n",
            "Image saved as result/facades/cmp_b0231.png\n",
            "Image saved as result/facades/cmp_b0241.png\n",
            "Image saved as result/facades/cmp_b0233.png\n",
            "Image saved as result/facades/cmp_b0246.png\n",
            "Image saved as result/facades/cmp_b0239.png\n",
            "Image saved as result/facades/cmp_b0242.png\n",
            "Image saved as result/facades/cmp_b0247.png\n",
            "Image saved as result/facades/cmp_b0235.png\n",
            "Image saved as result/facades/cmp_b0234.png\n",
            "Image saved as result/facades/cmp_b0250.png\n",
            "Image saved as result/facades/cmp_b0260.png\n",
            "Image saved as result/facades/cmp_b0286.png\n",
            "Image saved as result/facades/cmp_b0272.png\n",
            "Image saved as result/facades/cmp_b0271.png\n",
            "Image saved as result/facades/cmp_b0275.png\n",
            "Image saved as result/facades/cmp_b0280.png\n",
            "Image saved as result/facades/cmp_b0258.png\n",
            "Image saved as result/facades/cmp_b0283.png\n",
            "Image saved as result/facades/cmp_b0285.png\n",
            "Image saved as result/facades/cmp_b0288.png\n",
            "Image saved as result/facades/cmp_b0298.png\n",
            "Image saved as result/facades/cmp_b0291.png\n",
            "Image saved as result/facades/cmp_b0294.png\n",
            "Image saved as result/facades/cmp_b0305.png\n",
            "Image saved as result/facades/cmp_b0302.png\n",
            "Image saved as result/facades/cmp_b0300.png\n",
            "Image saved as result/facades/cmp_b0293.png\n",
            "Image saved as result/facades/cmp_b0290.png\n",
            "Image saved as result/facades/cmp_b0292.png\n",
            "Image saved as result/facades/cmp_b0306.png\n",
            "Image saved as result/facades/cmp_b0331.png\n",
            "Image saved as result/facades/cmp_b0001.png\n",
            "Image saved as result/facades/cmp_b0002.png\n",
            "Image saved as result/facades/cmp_b0004.png\n",
            "Image saved as result/facades/cmp_b0003.png\n",
            "Image saved as result/facades/cmp_b0013.png\n",
            "Image saved as result/facades/cmp_b0028.png\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}